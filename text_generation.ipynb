{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
    "\n",
    "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:26:43.689951Z",
     "iopub.status.busy": "2024-12-07T16:26:43.689094Z",
     "iopub.status.idle": "2024-12-07T16:26:43.694613Z",
     "shell.execute_reply": "2024-12-07T16:26:43.693761Z",
     "shell.execute_reply.started": "2024-12-07T16:26:43.689917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import string\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:04:12.598093Z",
     "iopub.status.busy": "2024-12-07T16:04:12.597742Z",
     "iopub.status.idle": "2024-12-07T16:04:12.684216Z",
     "shell.execute_reply": "2024-12-07T16:04:12.683352Z",
     "shell.execute_reply.started": "2024-12-07T16:04:12.598062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device is available\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('{} device is available'.format(device))\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPenWOy01Ooa",
    "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
   },
   "source": [
    "#### 1. Загрузка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:05:56.057810Z",
     "iopub.status.busy": "2024-12-07T16:05:56.057359Z",
     "iopub.status.idle": "2024-12-07T16:05:57.298063Z",
     "shell.execute_reply": "2024-12-07T16:05:57.297009Z",
     "shell.execute_reply.started": "2024-12-07T16:05:56.057768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-07 16:05:57--  https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262521 (256K) [text/plain]\n",
      "Saving to: 'onegin.txt'\n",
      "\n",
      "onegin.txt          100%[===================>] 256.37K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-12-07 16:05:57 (9.38 MB/s) - 'onegin.txt' saved [262521/262521]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
    "    \n",
    "with open('onegin.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XQYpmGfR_gJ8"
   },
   "source": [
    "#### 2. Построение словаря и предобработка текста\n",
    "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:06:02.463787Z",
     "iopub.status.busy": "2024-12-07T16:06:02.463395Z",
     "iopub.status.idle": "2024-12-07T16:06:02.493624Z",
     "shell.execute_reply": "2024-12-07T16:06:02.492663Z",
     "shell.execute_reply.started": "2024-12-07T16:06:02.463755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "tokens = sorted(set(text.lower())) + ['<sos>']\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "assert num_tokens == 84, \"Check the tokenization process\"\n",
    "\n",
    "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
    "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
    "\n",
    "print(\"Seems fine!\")\n",
    "\n",
    "\n",
    "text_encoded = [token_to_idx[x] for x in text]\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
    "\n",
    "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
    "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
    "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
    "\n",
    "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
    "\n",
    "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:28:00.956085Z",
     "iopub.status.busy": "2024-12-07T16:28:00.955422Z",
     "iopub.status.idle": "2024-12-07T16:28:00.964614Z",
     "shell.execute_reply": "2024-12-07T16:28:00.963723Z",
     "shell.execute_reply.started": "2024-12-07T16:28:00.956053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "batch_size = 256\n",
    "seq_length = 100\n",
    "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>']\n",
    "\n",
    "def generate_chunk():\n",
    "    global text_encoded, start_column, batch_size, seq_length\n",
    "\n",
    "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
    "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
    "    yield np.hstack((start_column, data))\n",
    "# __________end of block__________    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример батча:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T16:28:04.155436Z",
     "iopub.status.busy": "2024-12-07T16:28:04.155095Z",
     "iopub.status.idle": "2024-12-07T16:28:04.164434Z",
     "shell.execute_reply": "2024-12-07T16:28:04.163573Z",
     "shell.execute_reply.started": "2024-12-07T16:28:04.155406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 64, 61, ..., 58, 50, 63],\n",
       "       [83,  5,  1, ...,  1, 45, 61],\n",
       "       [83, 57, 53, ..., 61, 52, 45],\n",
       "       ...,\n",
       "       [83, 45, 57, ..., 51,  5,  1],\n",
       "       [83, 59, 58, ..., 47,  1, 60],\n",
       "       [83, 59, 56, ..., 58, 73,  1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_chunk())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вам предстоит написать код для обучения модели и генерации текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:10:22.973536Z",
     "iopub.status.busy": "2024-12-07T17:10:22.973155Z",
     "iopub.status.idle": "2024-12-07T17:10:22.978427Z",
     "shell.execute_reply": "2024-12-07T17:10:22.977604Z",
     "shell.execute_reply.started": "2024-12-07T17:10:22.973485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# your beautiful experiments here\n",
    "\n",
    "# Параметры модели\n",
    "vocab_size = len(idx_to_token)        # Размер словаря\n",
    "embed_size = 128                      # Размер эмбеддингов\n",
    "hidden_size = 256                     # Размер скрытого слоя\n",
    "num_layers = 1                        # Количество слоев RNN\n",
    "\n",
    "# Гиперпараметры обучения\n",
    "num_epochs = 100                       # Количество эпох\n",
    "learning_rate = 0.002                 # Скорость обучения\n",
    "clip_grad = 5.0                       # Ограничение градиентов для предотвращения исчезновения или взрыва градиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:10:26.085158Z",
     "iopub.status.busy": "2024-12-07T17:10:26.084812Z",
     "iopub.status.idle": "2024-12-07T17:10:26.091754Z",
     "shell.execute_reply": "2024-12-07T17:10:26.090913Z",
     "shell.execute_reply.started": "2024-12-07T17:10:26.085127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Векторизация входных символов\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Рекуррентный слой\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Полносвязный слой для предсказания следующего символа\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length)\n",
    "        hidden: (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        out, hidden = self.rnn(embedded, hidden)  # out: (batch_size, seq_length, hidden_size)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)  # (batch_size * seq_length, hidden_size)\n",
    "        out = self.fc(out)  # (batch_size * seq_length, vocab_size)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Инициализация скрытого состояния нулями\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:10:30.132179Z",
     "iopub.status.busy": "2024-12-07T17:10:30.131833Z",
     "iopub.status.idle": "2024-12-07T17:10:30.140195Z",
     "shell.execute_reply": "2024-12-07T17:10:30.139575Z",
     "shell.execute_reply.started": "2024-12-07T17:10:30.132149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "model = VanillaRNN(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Функция потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Оптимизатор\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:10:33.126823Z",
     "iopub.status.busy": "2024-12-07T17:10:33.126001Z",
     "iopub.status.idle": "2024-12-07T17:10:34.524048Z",
     "shell.execute_reply": "2024-12-07T17:10:34.523087Z",
     "shell.execute_reply.started": "2024-12-07T17:10:33.126788Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха [1/100], Потери: 4.4732\n",
      "Эпоха [2/100], Потери: 4.2402\n",
      "Эпоха [3/100], Потери: 3.9724\n",
      "Эпоха [4/100], Потери: 3.6177\n",
      "Эпоха [5/100], Потери: 3.3431\n",
      "Эпоха [6/100], Потери: 3.2370\n",
      "Эпоха [7/100], Потери: 3.1704\n",
      "Эпоха [8/100], Потери: 3.1072\n",
      "Эпоха [9/100], Потери: 3.0590\n",
      "Эпоха [10/100], Потери: 3.0213\n",
      "Эпоха [11/100], Потери: 2.9795\n",
      "Эпоха [12/100], Потери: 2.9114\n",
      "Эпоха [13/100], Потери: 2.8950\n",
      "Эпоха [14/100], Потери: 2.8733\n",
      "Эпоха [15/100], Потери: 2.8339\n",
      "Эпоха [16/100], Потери: 2.8002\n",
      "Эпоха [17/100], Потери: 2.7786\n",
      "Эпоха [18/100], Потери: 2.7614\n",
      "Эпоха [19/100], Потери: 2.7281\n",
      "Эпоха [20/100], Потери: 2.7388\n",
      "Эпоха [21/100], Потери: 2.6964\n",
      "Эпоха [22/100], Потери: 2.7007\n",
      "Эпоха [23/100], Потери: 2.6731\n",
      "Эпоха [24/100], Потери: 2.6553\n",
      "Эпоха [25/100], Потери: 2.6300\n",
      "Эпоха [26/100], Потери: 2.6103\n",
      "Эпоха [27/100], Потери: 2.6027\n",
      "Эпоха [28/100], Потери: 2.6014\n",
      "Эпоха [29/100], Потери: 2.5948\n",
      "Эпоха [30/100], Потери: 2.5915\n",
      "Эпоха [31/100], Потери: 2.5559\n",
      "Эпоха [32/100], Потери: 2.5454\n",
      "Эпоха [33/100], Потери: 2.5329\n",
      "Эпоха [34/100], Потери: 2.5377\n",
      "Эпоха [35/100], Потери: 2.5222\n",
      "Эпоха [36/100], Потери: 2.5512\n",
      "Эпоха [37/100], Потери: 2.5178\n",
      "Эпоха [38/100], Потери: 2.5397\n",
      "Эпоха [39/100], Потери: 2.5262\n",
      "Эпоха [40/100], Потери: 2.4815\n",
      "Эпоха [41/100], Потери: 2.4842\n",
      "Эпоха [42/100], Потери: 2.4782\n",
      "Эпоха [43/100], Потери: 2.4657\n",
      "Эпоха [44/100], Потери: 2.4493\n",
      "Эпоха [45/100], Потери: 2.4556\n",
      "Эпоха [46/100], Потери: 2.4470\n",
      "Эпоха [47/100], Потери: 2.4489\n",
      "Эпоха [48/100], Потери: 2.4564\n",
      "Эпоха [49/100], Потери: 2.4616\n",
      "Эпоха [50/100], Потери: 2.4334\n",
      "Эпоха [51/100], Потери: 2.4216\n",
      "Эпоха [52/100], Потери: 2.4010\n",
      "Эпоха [53/100], Потери: 2.4051\n",
      "Эпоха [54/100], Потери: 2.3999\n",
      "Эпоха [55/100], Потери: 2.3895\n",
      "Эпоха [56/100], Потери: 2.4060\n",
      "Эпоха [57/100], Потери: 2.4028\n",
      "Эпоха [58/100], Потери: 2.3947\n",
      "Эпоха [59/100], Потери: 2.3902\n",
      "Эпоха [60/100], Потери: 2.3754\n",
      "Эпоха [61/100], Потери: 2.3585\n",
      "Эпоха [62/100], Потери: 2.3602\n",
      "Эпоха [63/100], Потери: 2.3564\n",
      "Эпоха [64/100], Потери: 2.3503\n",
      "Эпоха [65/100], Потери: 2.3563\n",
      "Эпоха [66/100], Потери: 2.3368\n",
      "Эпоха [67/100], Потери: 2.3772\n",
      "Эпоха [68/100], Потери: 2.3284\n",
      "Эпоха [69/100], Потери: 2.3276\n",
      "Эпоха [70/100], Потери: 2.3400\n",
      "Эпоха [71/100], Потери: 2.3209\n",
      "Эпоха [72/100], Потери: 2.3352\n",
      "Эпоха [73/100], Потери: 2.3078\n",
      "Эпоха [74/100], Потери: 2.3056\n",
      "Эпоха [75/100], Потери: 2.2886\n",
      "Эпоха [76/100], Потери: 2.3061\n",
      "Эпоха [77/100], Потери: 2.3525\n",
      "Эпоха [78/100], Потери: 2.3365\n",
      "Эпоха [79/100], Потери: 2.2911\n",
      "Эпоха [80/100], Потери: 2.3328\n",
      "Эпоха [81/100], Потери: 2.3002\n",
      "Эпоха [82/100], Потери: 2.3102\n",
      "Эпоха [83/100], Потери: 2.3005\n",
      "Эпоха [84/100], Потери: 2.2641\n",
      "Эпоха [85/100], Потери: 2.2822\n",
      "Эпоха [86/100], Потери: 2.2563\n",
      "Эпоха [87/100], Потери: 2.2838\n",
      "Эпоха [88/100], Потери: 2.2771\n",
      "Эпоха [89/100], Потери: 2.2640\n",
      "Эпоха [90/100], Потери: 2.2880\n",
      "Эпоха [91/100], Потери: 2.2652\n",
      "Эпоха [92/100], Потери: 2.2999\n",
      "Эпоха [93/100], Потери: 2.2540\n",
      "Эпоха [94/100], Потери: 2.2663\n",
      "Эпоха [95/100], Потери: 2.2363\n",
      "Эпоха [96/100], Потери: 2.2309\n",
      "Эпоха [97/100], Потери: 2.2409\n",
      "Эпоха [98/100], Потери: 2.2331\n",
      "Эпоха [99/100], Потери: 2.2449\n",
      "Эпоха [100/100], Потери: 2.2745\n"
     ]
    }
   ],
   "source": [
    "model.train()  # Переводим модель в режим обучения\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Генерация батча данных\n",
    "    batch = next(generate_chunk())  # (batch_size, seq_length + 1)\n",
    "    inputs = batch[:, :-1]         # Входные символы\n",
    "    targets = batch[:, 1:].reshape(-1)  # Целевые символы (сдвинуто на один вперед)\n",
    "\n",
    "    # Перенос данных на устройство\n",
    "    inputs = torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "    # Инициализация скрытого состояния\n",
    "    hidden = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "    # Обнуление градиентов\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Прямой проход\n",
    "    outputs, hidden = model(inputs, hidden.detach())\n",
    "\n",
    "    # Вычисление потерь\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Ограничение градиентов\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "    # Обновление параметров\n",
    "    optimizer.step()\n",
    "\n",
    "    # Печать информации об обучении\n",
    "    print(f\"Эпоха [{epoch}/{num_epochs}], Потери: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:10:37.743555Z",
     "iopub.status.busy": "2024-12-07T17:10:37.743233Z",
     "iopub.status.idle": "2024-12-07T17:10:37.747779Z",
     "shell.execute_reply": "2024-12-07T17:10:37.746812Z",
     "shell.execute_reply.started": "2024-12-07T17:10:37.743513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# start_phrase = \"Онегин сказал: \"\n",
    "# generated_text = generate_text(model, start_phrase, generate_length=100)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:14:19.427171Z",
     "iopub.status.busy": "2024-12-07T17:14:19.426745Z",
     "iopub.status.idle": "2024-12-07T17:14:19.435262Z",
     "shell.execute_reply": "2024-12-07T17:14:19.434218Z",
     "shell.execute_reply.started": "2024-12-07T17:14:19.427140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    if seed_phrase is not None:\n",
    "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
    "    else: \n",
    "        x_sequence = [token_to_idx['<sos>']]\n",
    "\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "            \n",
    "    # your code here\n",
    "    # Инициализация скрытого состояния\n",
    "    hidden = char_rnn.init_hidden(x_sequence.size(0)).to(device)\n",
    "    char_rnn.eval()  # Переключение в режим оценки\n",
    "\n",
    "    # Прогоняем начальную последовательность через модель\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            output, hidden = char_rnn(x_sequence[:, -1].view(1, -1), hidden)  # Берем последний символ\n",
    "            output_dist = output.view(-1).div(temperature).exp()  # Применяем температуру\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]  # Выбор следующего символа\n",
    "            \n",
    "            x_sequence = torch.cat((x_sequence, top_i.view(1, 1)), dim=1)  # Добавляем новый символ к последовательности\n",
    "\n",
    "\n",
    "    \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.cpu().data.numpy()[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:14:50.049599Z",
     "iopub.status.busy": "2024-12-07T17:14:50.049000Z",
     "iopub.status.idle": "2024-12-07T17:14:50.297353Z",
     "shell.execute_reply": "2024-12-07T17:14:50.296463Z",
     "shell.execute_reply.started": "2024-12-07T17:14:50.049546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> мой дядя самых честных правилюбы\n",
      "са кристо о ведный ране милачит плестые сзнежно е\n",
      "да крылась узарых,\n",
      "немил наеть медной,\n",
      "и мни тае всё ость овадежна, вопот наворлит, наздой никам;\n",
      "\n",
      "\n",
      "xxiiii\n",
      "\n",
      "на поторьнай поэная петтый своть к подриго.\n",
      "сниша полки,\n",
      "черь.\n",
      "он но в скуглянь овото нена насоны что безвом\n",
      "и квола…\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xxvi\n",
      "\n",
      "не быльные хонимы\n",
      "и вы на на дорой\n",
      "поакнодов нам глеталися в онать на стади в ракот.\n",
      "о породил ей жерди стракиветь.\n",
      "отереских звой свренн.\n",
      "\n",
      "\n",
      "\n",
      "xviv\n",
      "\n",
      "но тновит, пернетсчей солубила.\n",
      "на двучев не, радомной изазьян\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
    "\n",
    "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:15:07.988978Z",
     "iopub.status.busy": "2024-12-07T17:15:07.988158Z",
     "iopub.status.idle": "2024-12-07T17:15:07.993222Z",
     "shell.execute_reply": "2024-12-07T17:15:07.992145Z",
     "shell.execute_reply.started": "2024-12-07T17:15:07.988940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed_phrase = ' мой дядя самых честных правил'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:18:13.731164Z",
     "iopub.status.busy": "2024-12-07T17:18:13.730537Z",
     "iopub.status.idle": "2024-12-07T17:18:16.015155Z",
     "shell.execute_reply": "2024-12-07T17:18:16.014173Z",
     "shell.execute_reply.started": "2024-12-07T17:18:13.731133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# generated_phrases = # your code here\n",
    "\n",
    "generated_phrases = [\n",
    "    generate_sample(\n",
    "        model,\n",
    "        ' мой дядя самых честных правил',\n",
    "        max_length=470,\n",
    "        temperature=1.\n",
    "    ).replace('<sos>', '')\n",
    "    for _ in range(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:18:16.016959Z",
     "iopub.status.busy": "2024-12-07T17:18:16.016664Z",
     "iopub.status.idle": "2024-12-07T17:18:16.022119Z",
     "shell.execute_reply": "2024-12-07T17:18:16.021104Z",
     "shell.execute_reply.started": "2024-12-07T17:18:16.016933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n"
     ]
    }
   ],
   "source": [
    "print([len(generated_phrases[i]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T17:18:22.004005Z",
     "iopub.status.busy": "2024-12-07T17:18:22.003168Z",
     "iopub.status.idle": "2024-12-07T17:18:22.011887Z",
     "shell.execute_reply": "2024-12-07T17:18:22.011017Z",
     "shell.execute_reply.started": "2024-12-07T17:18:22.003965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "import json\n",
    "if 'generated_phrases' not in locals():\n",
    "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
    "\n",
    "for phrase in generated_phrases:\n",
    "\n",
    "    if not isinstance(phrase, str):\n",
    "        raise ValueError(\"The generated phrase should be a string\")\n",
    "\n",
    "    if len(phrase) != 500:\n",
    "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
    "\n",
    "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
    "    \n",
    "\n",
    "submission_dict = {\n",
    "    'token_to_idx': token_to_idx,\n",
    "    'generated_phrases': generated_phrases\n",
    "}\n",
    "\n",
    "with open('submission_dict.json', 'w') as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print('File saved to `submission_dict.json`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6249978,
     "sourceId": 10127677,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
